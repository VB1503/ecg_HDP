{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/rameshavinash94/Cardiovascular-Detection-using-ECG-images/blob/main/Merging_Scaled_1D_%26_Trying_Different_CLassification_ML_Models_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CIqvxE_BKjV6"
   },
   "source": [
    "### IMPORTING LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting natsort\n",
      "  Obtaining dependency information for natsort from https://files.pythonhosted.org/packages/ef/82/7a9d0550484a62c6da82858ee9419f3dd1ccc9aa1c26a1e43da3ecd20b0d/natsort-8.4.0-py3-none-any.whl.metadata\n",
      "  Downloading natsort-8.4.0-py3-none-any.whl.metadata (21 kB)\n",
      "Downloading natsort-8.4.0-py3-none-any.whl (38 kB)\n",
      "Installing collected packages: natsort\n",
      "Successfully installed natsort-8.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install natsort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "C82bu1OsKnfC"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "from natsort import natsorted\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXx7SoC7qyRv"
   },
   "source": [
    "### **WORKING ON COMBING MULTIPLE LEAD FILES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "sI0sFzAqPP8g"
   },
   "outputs": [],
   "source": [
    "#creating list to store file_names\n",
    "NORMAL_=[]\n",
    "MI_=[]\n",
    "PMI_=[]\n",
    "HB_=[]\n",
    "\n",
    "normal = 'D:\\Acamedic Mini Projects 2023\\Cardiovascular-Detection-using-ECG-images\\preprocessed_1d/NORMAL'\n",
    "abnormal = 'D:\\Acamedic Mini Projects 2023\\Cardiovascular-Detection-using-ECG-images\\preprocessed_1d/AHB'\n",
    "MI = 'D:\\Acamedic Mini Projects 2023\\Cardiovascular-Detection-using-ECG-images\\preprocessed_1d/MI'\n",
    "MI_history = 'D:\\Acamedic Mini Projects 2023\\Cardiovascular-Detection-using-ECG-images\\preprocessed_1d/PM'\n",
    "\n",
    "Types_ECG = {'normal':normal,'Abnormal_hear_beat':abnormal,'MI':MI,'History_MI':MI_history}\n",
    "\n",
    "for types,folder in Types_ECG.items():\n",
    "  for files in os.listdir(folder):\n",
    "    if types=='normal':\n",
    "      NORMAL_.append(files)\n",
    "    elif types=='Abnormal_hear_beat':\n",
    "      HB_.append(files)\n",
    "    elif types=='MI':\n",
    "      MI_.append(files)\n",
    "    elif types=='History_MI':\n",
    "      PMI_.append(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AJL9qAFSUOsN",
    "outputId": "9a4817f5-ae50-4aaf-96fe-8cc945289ad7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaled_data_1D_1.csv',\n",
       " 'scaled_data_1D_2.csv',\n",
       " 'scaled_data_1D_3.csv',\n",
       " 'scaled_data_1D_4.csv',\n",
       " 'scaled_data_1D_5.csv',\n",
       " 'scaled_data_1D_6.csv',\n",
       " 'scaled_data_1D_7.csv',\n",
       " 'scaled_data_1D_8.csv',\n",
       " 'scaled_data_1D_9.csv',\n",
       " 'scaled_data_1D_10.csv',\n",
       " 'scaled_data_1D_11.csv',\n",
       " 'scaled_data_1D_12.csv',\n",
       " 'scaled_data_1D_13.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NORMAL_ = natsorted(NORMAL_)\n",
    "NORMAL_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BRbNR2HEUkvU",
    "outputId": "0cd461c0-5fe2-463a-cbf5-6325662d9011"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaled_data_1D_1.csv',\n",
       " 'scaled_data_1D_2.csv',\n",
       " 'scaled_data_1D_3.csv',\n",
       " 'scaled_data_1D_4.csv',\n",
       " 'scaled_data_1D_5.csv',\n",
       " 'scaled_data_1D_6.csv',\n",
       " 'scaled_data_1D_7.csv',\n",
       " 'scaled_data_1D_8.csv',\n",
       " 'scaled_data_1D_9.csv',\n",
       " 'scaled_data_1D_10.csv',\n",
       " 'scaled_data_1D_11.csv',\n",
       " 'scaled_data_1D_12.csv',\n",
       " 'scaled_data_1D_13.csv']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MI_ = natsorted(MI_)\n",
    "MI_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A4LMfTU3UuCM",
    "outputId": "0c127e8e-62b7-4015-e3ee-a34860e4f5ec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaled_data_1D_1.csv',\n",
       " 'scaled_data_1D_2.csv',\n",
       " 'scaled_data_1D_3.csv',\n",
       " 'scaled_data_1D_4.csv',\n",
       " 'scaled_data_1D_5.csv',\n",
       " 'scaled_data_1D_6.csv',\n",
       " 'scaled_data_1D_7.csv',\n",
       " 'scaled_data_1D_8.csv',\n",
       " 'scaled_data_1D_9.csv',\n",
       " 'scaled_data_1D_10.csv',\n",
       " 'scaled_data_1D_11.csv',\n",
       " 'scaled_data_1D_12.csv',\n",
       " 'scaled_data_1D_13.csv']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PMI_ = natsorted(PMI_)\n",
    "PMI_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OinhdMKtU5mf",
    "outputId": "52dd393b-76c2-44a9-e64e-4d428380ed8d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaled_data_1D_1.csv',\n",
       " 'scaled_data_1D_2.csv',\n",
       " 'scaled_data_1D_3.csv',\n",
       " 'scaled_data_1D_4.csv',\n",
       " 'scaled_data_1D_5.csv',\n",
       " 'scaled_data_1D_6.csv',\n",
       " 'scaled_data_1D_7.csv',\n",
       " 'scaled_data_1D_8.csv',\n",
       " 'scaled_data_1D_9.csv',\n",
       " 'scaled_data_1D_10.csv',\n",
       " 'scaled_data_1D_11.csv',\n",
       " 'scaled_data_1D_12.csv',\n",
       " 'scaled_data_1D_13.csv']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HB_ = natsorted(HB_)\n",
    "HB_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5T4Ay50ch6Ru"
   },
   "source": [
    "#### **COMBINED CSV OF EACH LEAD(1-12) FROM ALL IMAGES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Ot65BTX2VF7Y"
   },
   "outputs": [],
   "source": [
    "#loop over and create combined csv files for each leads.\n",
    "for x in range(len(MI_)):\n",
    "  df1=pd.read_csv('../preprocessed_1d/NORMAL/{}'.format(NORMAL_[x]))\n",
    "  df2=pd.read_csv('../preprocessed_1d/AHB/{}'.format(HB_[x]))\n",
    "  df3=pd.read_csv('../preprocessed_1d/MI/{}'.format(MI_[x]))\n",
    "  df4=pd.read_csv('../preprocessed_1d/PM/{}'.format(PMI_[x]))\n",
    "  final_df = pd.concat([df1,df2,df3,df4],ignore_index=True)\n",
    "  final_df.to_csv('../preprocessed_1d/Combined_IDLead_{}.csv'.format(x+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RyTYwqJSX8Vh",
    "outputId": "b1c922e9-83f4-4d31-e5cd-d5b18a78acf0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['No', 'HB', 'MI', 'PM'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now reading just lead1\n",
    "df=pd.read_csv('../preprocessed_1d/Combined_IDLead_1.csv')\n",
    "df['Target'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "nbSKpUc2angF"
   },
   "outputs": [],
   "source": [
    "df.drop(columns=['Unnamed: 0'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "id": "fX_uYtekhL_z",
    "outputId": "a6d14a8e-a75f-4b1b-f39f-7830cba4c581"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.728449</td>\n",
       "      <td>0.680755</td>\n",
       "      <td>0.619010</td>\n",
       "      <td>0.645367</td>\n",
       "      <td>0.681570</td>\n",
       "      <td>0.732488</td>\n",
       "      <td>0.758448</td>\n",
       "      <td>0.750660</td>\n",
       "      <td>0.728282</td>\n",
       "      <td>0.707928</td>\n",
       "      <td>...</td>\n",
       "      <td>0.637260</td>\n",
       "      <td>0.664539</td>\n",
       "      <td>0.667226</td>\n",
       "      <td>0.637064</td>\n",
       "      <td>0.593287</td>\n",
       "      <td>0.545503</td>\n",
       "      <td>0.515049</td>\n",
       "      <td>0.563257</td>\n",
       "      <td>0.633581</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.957972</td>\n",
       "      <td>0.950695</td>\n",
       "      <td>0.941024</td>\n",
       "      <td>0.930501</td>\n",
       "      <td>0.913601</td>\n",
       "      <td>0.892244</td>\n",
       "      <td>0.868016</td>\n",
       "      <td>0.855127</td>\n",
       "      <td>0.835307</td>\n",
       "      <td>0.798640</td>\n",
       "      <td>...</td>\n",
       "      <td>0.778790</td>\n",
       "      <td>0.806883</td>\n",
       "      <td>0.818640</td>\n",
       "      <td>0.842472</td>\n",
       "      <td>0.866740</td>\n",
       "      <td>0.884152</td>\n",
       "      <td>0.897196</td>\n",
       "      <td>0.911293</td>\n",
       "      <td>0.922903</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.611084</td>\n",
       "      <td>0.661575</td>\n",
       "      <td>0.695790</td>\n",
       "      <td>0.741113</td>\n",
       "      <td>0.716666</td>\n",
       "      <td>0.595794</td>\n",
       "      <td>0.425022</td>\n",
       "      <td>0.286457</td>\n",
       "      <td>0.425022</td>\n",
       "      <td>0.611384</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042690</td>\n",
       "      <td>0.165850</td>\n",
       "      <td>0.363445</td>\n",
       "      <td>0.549460</td>\n",
       "      <td>0.539346</td>\n",
       "      <td>0.522272</td>\n",
       "      <td>0.491668</td>\n",
       "      <td>0.454949</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.839213</td>\n",
       "      <td>0.861690</td>\n",
       "      <td>0.866457</td>\n",
       "      <td>0.865756</td>\n",
       "      <td>0.855027</td>\n",
       "      <td>0.855606</td>\n",
       "      <td>0.845561</td>\n",
       "      <td>0.843187</td>\n",
       "      <td>0.846784</td>\n",
       "      <td>0.824438</td>\n",
       "      <td>...</td>\n",
       "      <td>0.789156</td>\n",
       "      <td>0.793622</td>\n",
       "      <td>0.787665</td>\n",
       "      <td>0.794515</td>\n",
       "      <td>0.796739</td>\n",
       "      <td>0.804063</td>\n",
       "      <td>0.809944</td>\n",
       "      <td>0.801814</td>\n",
       "      <td>0.777322</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.917753</td>\n",
       "      <td>0.924369</td>\n",
       "      <td>0.873765</td>\n",
       "      <td>0.791381</td>\n",
       "      <td>0.699513</td>\n",
       "      <td>0.604927</td>\n",
       "      <td>0.500312</td>\n",
       "      <td>0.446012</td>\n",
       "      <td>0.528910</td>\n",
       "      <td>0.634068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200676</td>\n",
       "      <td>0.300147</td>\n",
       "      <td>0.407225</td>\n",
       "      <td>0.507346</td>\n",
       "      <td>0.605953</td>\n",
       "      <td>0.699309</td>\n",
       "      <td>0.790334</td>\n",
       "      <td>0.856593</td>\n",
       "      <td>0.849957</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>0.874246</td>\n",
       "      <td>0.877014</td>\n",
       "      <td>0.864280</td>\n",
       "      <td>0.860505</td>\n",
       "      <td>0.871349</td>\n",
       "      <td>0.912404</td>\n",
       "      <td>0.958148</td>\n",
       "      <td>0.977826</td>\n",
       "      <td>0.956314</td>\n",
       "      <td>0.926773</td>\n",
       "      <td>...</td>\n",
       "      <td>0.908312</td>\n",
       "      <td>0.926328</td>\n",
       "      <td>0.898749</td>\n",
       "      <td>0.855709</td>\n",
       "      <td>0.823132</td>\n",
       "      <td>0.815458</td>\n",
       "      <td>0.818083</td>\n",
       "      <td>0.829300</td>\n",
       "      <td>0.822382</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>0.829815</td>\n",
       "      <td>0.832084</td>\n",
       "      <td>0.852396</td>\n",
       "      <td>0.909665</td>\n",
       "      <td>0.988242</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923323</td>\n",
       "      <td>0.821865</td>\n",
       "      <td>0.721302</td>\n",
       "      <td>0.612039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.429721</td>\n",
       "      <td>0.531567</td>\n",
       "      <td>0.642137</td>\n",
       "      <td>0.742063</td>\n",
       "      <td>0.833042</td>\n",
       "      <td>0.814867</td>\n",
       "      <td>0.777622</td>\n",
       "      <td>0.760714</td>\n",
       "      <td>0.759294</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>0.469048</td>\n",
       "      <td>0.417983</td>\n",
       "      <td>0.362322</td>\n",
       "      <td>0.351995</td>\n",
       "      <td>0.391493</td>\n",
       "      <td>0.418305</td>\n",
       "      <td>0.440135</td>\n",
       "      <td>0.444598</td>\n",
       "      <td>0.460402</td>\n",
       "      <td>0.506810</td>\n",
       "      <td>...</td>\n",
       "      <td>0.408587</td>\n",
       "      <td>0.401864</td>\n",
       "      <td>0.387069</td>\n",
       "      <td>0.359590</td>\n",
       "      <td>0.325879</td>\n",
       "      <td>0.288894</td>\n",
       "      <td>0.293521</td>\n",
       "      <td>0.344504</td>\n",
       "      <td>0.399012</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926</th>\n",
       "      <td>0.682510</td>\n",
       "      <td>0.682286</td>\n",
       "      <td>0.641051</td>\n",
       "      <td>0.620212</td>\n",
       "      <td>0.608210</td>\n",
       "      <td>0.576331</td>\n",
       "      <td>0.603596</td>\n",
       "      <td>0.645714</td>\n",
       "      <td>0.677964</td>\n",
       "      <td>0.720297</td>\n",
       "      <td>...</td>\n",
       "      <td>0.452247</td>\n",
       "      <td>0.450421</td>\n",
       "      <td>0.439278</td>\n",
       "      <td>0.439086</td>\n",
       "      <td>0.394417</td>\n",
       "      <td>0.441650</td>\n",
       "      <td>0.473909</td>\n",
       "      <td>0.539199</td>\n",
       "      <td>0.547146</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927</th>\n",
       "      <td>0.792175</td>\n",
       "      <td>0.815695</td>\n",
       "      <td>0.819518</td>\n",
       "      <td>0.820559</td>\n",
       "      <td>0.847985</td>\n",
       "      <td>0.880933</td>\n",
       "      <td>0.902061</td>\n",
       "      <td>0.878266</td>\n",
       "      <td>0.838806</td>\n",
       "      <td>0.811795</td>\n",
       "      <td>...</td>\n",
       "      <td>0.737351</td>\n",
       "      <td>0.778845</td>\n",
       "      <td>0.805446</td>\n",
       "      <td>0.782640</td>\n",
       "      <td>0.751236</td>\n",
       "      <td>0.741331</td>\n",
       "      <td>0.718790</td>\n",
       "      <td>0.714504</td>\n",
       "      <td>0.691004</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>928 rows × 256 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "0    0.728449  0.680755  0.619010  0.645367  0.681570  0.732488  0.758448   \n",
       "1    0.957972  0.950695  0.941024  0.930501  0.913601  0.892244  0.868016   \n",
       "2    0.611084  0.661575  0.695790  0.741113  0.716666  0.595794  0.425022   \n",
       "3    0.839213  0.861690  0.866457  0.865756  0.855027  0.855606  0.845561   \n",
       "4    0.917753  0.924369  0.873765  0.791381  0.699513  0.604927  0.500312   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "923  0.874246  0.877014  0.864280  0.860505  0.871349  0.912404  0.958148   \n",
       "924  0.829815  0.832084  0.852396  0.909665  0.988242  1.000000  0.923323   \n",
       "925  0.469048  0.417983  0.362322  0.351995  0.391493  0.418305  0.440135   \n",
       "926  0.682510  0.682286  0.641051  0.620212  0.608210  0.576331  0.603596   \n",
       "927  0.792175  0.815695  0.819518  0.820559  0.847985  0.880933  0.902061   \n",
       "\n",
       "            7         8         9  ...       246       247       248  \\\n",
       "0    0.750660  0.728282  0.707928  ...  0.637260  0.664539  0.667226   \n",
       "1    0.855127  0.835307  0.798640  ...  0.778790  0.806883  0.818640   \n",
       "2    0.286457  0.425022  0.611384  ...  0.000000  0.042690  0.165850   \n",
       "3    0.843187  0.846784  0.824438  ...  0.789156  0.793622  0.787665   \n",
       "4    0.446012  0.528910  0.634068  ...  0.200676  0.300147  0.407225   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "923  0.977826  0.956314  0.926773  ...  0.908312  0.926328  0.898749   \n",
       "924  0.821865  0.721302  0.612039  ...  0.429721  0.531567  0.642137   \n",
       "925  0.444598  0.460402  0.506810  ...  0.408587  0.401864  0.387069   \n",
       "926  0.645714  0.677964  0.720297  ...  0.452247  0.450421  0.439278   \n",
       "927  0.878266  0.838806  0.811795  ...  0.737351  0.778845  0.805446   \n",
       "\n",
       "          249       250       251       252       253       254  target  \n",
       "0    0.637064  0.593287  0.545503  0.515049  0.563257  0.633581       2  \n",
       "1    0.842472  0.866740  0.884152  0.897196  0.911293  0.922903       2  \n",
       "2    0.363445  0.549460  0.539346  0.522272  0.491668  0.454949       2  \n",
       "3    0.794515  0.796739  0.804063  0.809944  0.801814  0.777322       2  \n",
       "4    0.507346  0.605953  0.699309  0.790334  0.856593  0.849957       2  \n",
       "..        ...       ...       ...       ...       ...       ...     ...  \n",
       "923  0.855709  0.823132  0.815458  0.818083  0.829300  0.822382       3  \n",
       "924  0.742063  0.833042  0.814867  0.777622  0.760714  0.759294       3  \n",
       "925  0.359590  0.325879  0.288894  0.293521  0.344504  0.399012       3  \n",
       "926  0.439086  0.394417  0.441650  0.473909  0.539199  0.547146       3  \n",
       "927  0.782640  0.751236  0.741331  0.718790  0.714504  0.691004       3  \n",
       "\n",
       "[928 rows x 256 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert Target column values as Numeric using ngroups\n",
    "encode_target_label = df.groupby('Target').ngroup().rename(\"target\").to_frame()\n",
    "test_final  = df.merge(encode_target_label, left_index=True, right_index=True)\n",
    "test_final.drop(columns=['Target'],inplace=True)\n",
    "test_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DRvRDvfrivtE"
   },
   "source": [
    "#### **PERFORM DIMENSIONALITY REDUCTION JUST FOR CHECKING/UNDERSTANDING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 912
    },
    "id": "scVPRQ3TZBaW",
    "outputId": "0b606399-2d4a-40a0-b9e3-cd7592305d67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance of each component: [1.76145888e-01 9.50265614e-02 6.99060614e-02 6.15960001e-02\n",
      " 5.34876630e-02 4.23664893e-02 3.68320213e-02 3.38541791e-02\n",
      " 3.00884979e-02 2.90396728e-02 2.64962509e-02 2.42272738e-02\n",
      " 2.10221030e-02 1.99751559e-02 1.77321042e-02 1.63016802e-02\n",
      " 1.53898622e-02 1.48412074e-02 1.33644825e-02 1.19674074e-02\n",
      " 1.16813409e-02 1.05807650e-02 9.68875480e-03 9.47385060e-03\n",
      " 8.65347748e-03 8.47506998e-03 7.93382172e-03 7.30163338e-03\n",
      " 6.76380665e-03 6.36886390e-03 6.02004791e-03 5.46823032e-03\n",
      " 5.31229911e-03 4.97821789e-03 4.74686092e-03 4.46081684e-03\n",
      " 4.21254684e-03 4.01200243e-03 3.87246476e-03 3.52519084e-03\n",
      " 3.37596894e-03 3.26978336e-03 3.08241145e-03 2.96423495e-03\n",
      " 2.73419816e-03 2.50965698e-03 2.35335480e-03 2.25665349e-03\n",
      " 2.20141761e-03 1.96782025e-03 1.74343954e-03 1.70982830e-03\n",
      " 1.57456047e-03 1.53704487e-03 1.36768435e-03 1.33167096e-03\n",
      " 1.26444173e-03 1.20053330e-03 1.18738749e-03 1.08864087e-03\n",
      " 1.02824532e-03 9.11484783e-04 7.89962329e-04 7.59785111e-04\n",
      " 6.49920864e-04 6.27833790e-04 6.04784065e-04 5.45886706e-04\n",
      " 5.32310102e-04 4.97728350e-04 4.78393192e-04 4.50404967e-04\n",
      " 4.26173486e-04 4.09392347e-04 3.92601459e-04 3.70241544e-04\n",
      " 3.66854817e-04 3.38846639e-04 3.08205738e-04 3.00634198e-04\n",
      " 2.80363385e-04 2.67851066e-04 2.49659419e-04 2.40637118e-04\n",
      " 2.11563314e-04 2.03936725e-04 1.99020898e-04 1.83136669e-04\n",
      " 1.66793973e-04 1.65881850e-04 1.55967222e-04 1.39073837e-04\n",
      " 1.25322607e-04 1.22194088e-04 1.13583050e-04 1.09473938e-04\n",
      " 1.03456924e-04 9.93052420e-05 9.84622026e-05 9.42729947e-05]\n",
      "\n",
      " Total Variance Explained: 99.8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.018578</td>\n",
       "      <td>1.148263</td>\n",
       "      <td>-0.589582</td>\n",
       "      <td>0.193617</td>\n",
       "      <td>0.047950</td>\n",
       "      <td>-0.309400</td>\n",
       "      <td>-0.161566</td>\n",
       "      <td>0.478471</td>\n",
       "      <td>0.972403</td>\n",
       "      <td>-0.031832</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013994</td>\n",
       "      <td>-0.002396</td>\n",
       "      <td>0.022853</td>\n",
       "      <td>0.010722</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>-0.000213</td>\n",
       "      <td>0.009859</td>\n",
       "      <td>-0.063037</td>\n",
       "      <td>-0.024770</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.098692</td>\n",
       "      <td>0.289832</td>\n",
       "      <td>-1.766388</td>\n",
       "      <td>1.076165</td>\n",
       "      <td>-0.261201</td>\n",
       "      <td>-0.820446</td>\n",
       "      <td>-0.474188</td>\n",
       "      <td>-0.515238</td>\n",
       "      <td>0.692389</td>\n",
       "      <td>1.501606</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017403</td>\n",
       "      <td>0.061122</td>\n",
       "      <td>-0.006753</td>\n",
       "      <td>0.011152</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.045316</td>\n",
       "      <td>0.034835</td>\n",
       "      <td>-0.036043</td>\n",
       "      <td>0.003199</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.275021</td>\n",
       "      <td>-0.451289</td>\n",
       "      <td>0.106750</td>\n",
       "      <td>-0.426415</td>\n",
       "      <td>0.066133</td>\n",
       "      <td>0.692474</td>\n",
       "      <td>0.634894</td>\n",
       "      <td>-0.035867</td>\n",
       "      <td>0.815855</td>\n",
       "      <td>-0.909473</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044420</td>\n",
       "      <td>-0.040892</td>\n",
       "      <td>-0.029098</td>\n",
       "      <td>0.121536</td>\n",
       "      <td>-0.046317</td>\n",
       "      <td>-0.019270</td>\n",
       "      <td>-0.022017</td>\n",
       "      <td>0.045859</td>\n",
       "      <td>0.007546</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.517085</td>\n",
       "      <td>1.662693</td>\n",
       "      <td>-1.021167</td>\n",
       "      <td>0.804267</td>\n",
       "      <td>-0.281985</td>\n",
       "      <td>0.518180</td>\n",
       "      <td>0.355748</td>\n",
       "      <td>-0.344235</td>\n",
       "      <td>-0.910867</td>\n",
       "      <td>-0.629517</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000275</td>\n",
       "      <td>0.041640</td>\n",
       "      <td>0.020649</td>\n",
       "      <td>-0.052982</td>\n",
       "      <td>-0.040261</td>\n",
       "      <td>-0.015403</td>\n",
       "      <td>-0.021176</td>\n",
       "      <td>0.022025</td>\n",
       "      <td>-0.034439</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.152840</td>\n",
       "      <td>-1.046283</td>\n",
       "      <td>0.351278</td>\n",
       "      <td>1.100381</td>\n",
       "      <td>-1.613642</td>\n",
       "      <td>1.484188</td>\n",
       "      <td>-0.113277</td>\n",
       "      <td>-0.251152</td>\n",
       "      <td>0.179023</td>\n",
       "      <td>-0.233104</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009806</td>\n",
       "      <td>0.054719</td>\n",
       "      <td>0.001238</td>\n",
       "      <td>0.032091</td>\n",
       "      <td>0.015511</td>\n",
       "      <td>-0.004524</td>\n",
       "      <td>0.043634</td>\n",
       "      <td>0.002376</td>\n",
       "      <td>-0.028378</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>-1.321884</td>\n",
       "      <td>2.153021</td>\n",
       "      <td>0.788596</td>\n",
       "      <td>-1.304253</td>\n",
       "      <td>0.458186</td>\n",
       "      <td>-0.859346</td>\n",
       "      <td>-0.069127</td>\n",
       "      <td>-0.392796</td>\n",
       "      <td>0.755732</td>\n",
       "      <td>-0.584050</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.037846</td>\n",
       "      <td>0.077444</td>\n",
       "      <td>-0.044552</td>\n",
       "      <td>-0.032070</td>\n",
       "      <td>0.007507</td>\n",
       "      <td>-0.026487</td>\n",
       "      <td>-0.051556</td>\n",
       "      <td>-0.004306</td>\n",
       "      <td>0.010902</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>-0.867163</td>\n",
       "      <td>-0.040504</td>\n",
       "      <td>0.940680</td>\n",
       "      <td>0.302648</td>\n",
       "      <td>-0.469672</td>\n",
       "      <td>-0.368255</td>\n",
       "      <td>1.065579</td>\n",
       "      <td>0.801522</td>\n",
       "      <td>0.690113</td>\n",
       "      <td>0.953008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012789</td>\n",
       "      <td>-0.093530</td>\n",
       "      <td>0.013706</td>\n",
       "      <td>-0.029583</td>\n",
       "      <td>-0.009552</td>\n",
       "      <td>0.057532</td>\n",
       "      <td>-0.007173</td>\n",
       "      <td>-0.068559</td>\n",
       "      <td>-0.016400</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>3.753012</td>\n",
       "      <td>0.841636</td>\n",
       "      <td>-0.317393</td>\n",
       "      <td>-0.296117</td>\n",
       "      <td>0.593769</td>\n",
       "      <td>-0.255474</td>\n",
       "      <td>-0.057091</td>\n",
       "      <td>-0.072048</td>\n",
       "      <td>0.664386</td>\n",
       "      <td>-0.837668</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008154</td>\n",
       "      <td>-0.005854</td>\n",
       "      <td>-0.005952</td>\n",
       "      <td>0.006260</td>\n",
       "      <td>-0.021991</td>\n",
       "      <td>0.067267</td>\n",
       "      <td>0.021478</td>\n",
       "      <td>-0.003742</td>\n",
       "      <td>-0.010129</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926</th>\n",
       "      <td>0.603083</td>\n",
       "      <td>0.126259</td>\n",
       "      <td>0.003433</td>\n",
       "      <td>0.283612</td>\n",
       "      <td>0.169559</td>\n",
       "      <td>-0.156326</td>\n",
       "      <td>-0.068399</td>\n",
       "      <td>-0.184308</td>\n",
       "      <td>0.461063</td>\n",
       "      <td>-0.002047</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057527</td>\n",
       "      <td>0.024187</td>\n",
       "      <td>-0.017715</td>\n",
       "      <td>-0.080232</td>\n",
       "      <td>-0.005021</td>\n",
       "      <td>0.033426</td>\n",
       "      <td>-0.057247</td>\n",
       "      <td>0.027725</td>\n",
       "      <td>0.062772</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927</th>\n",
       "      <td>-1.452945</td>\n",
       "      <td>1.233599</td>\n",
       "      <td>0.439472</td>\n",
       "      <td>0.278517</td>\n",
       "      <td>0.165928</td>\n",
       "      <td>-0.171830</td>\n",
       "      <td>-0.075000</td>\n",
       "      <td>0.033859</td>\n",
       "      <td>0.004444</td>\n",
       "      <td>-0.687583</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010228</td>\n",
       "      <td>-0.031783</td>\n",
       "      <td>0.075114</td>\n",
       "      <td>-0.062239</td>\n",
       "      <td>0.010940</td>\n",
       "      <td>-0.072666</td>\n",
       "      <td>0.006255</td>\n",
       "      <td>-0.057147</td>\n",
       "      <td>-0.023021</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>928 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "0    1.018578  1.148263 -0.589582  0.193617  0.047950 -0.309400 -0.161566   \n",
       "1   -1.098692  0.289832 -1.766388  1.076165 -0.261201 -0.820446 -0.474188   \n",
       "2    0.275021 -0.451289  0.106750 -0.426415  0.066133  0.692474  0.634894   \n",
       "3   -1.517085  1.662693 -1.021167  0.804267 -0.281985  0.518180  0.355748   \n",
       "4   -0.152840 -1.046283  0.351278  1.100381 -1.613642  1.484188 -0.113277   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "923 -1.321884  2.153021  0.788596 -1.304253  0.458186 -0.859346 -0.069127   \n",
       "924 -0.867163 -0.040504  0.940680  0.302648 -0.469672 -0.368255  1.065579   \n",
       "925  3.753012  0.841636 -0.317393 -0.296117  0.593769 -0.255474 -0.057091   \n",
       "926  0.603083  0.126259  0.003433  0.283612  0.169559 -0.156326 -0.068399   \n",
       "927 -1.452945  1.233599  0.439472  0.278517  0.165928 -0.171830 -0.075000   \n",
       "\n",
       "            7         8         9  ...        91        92        93  \\\n",
       "0    0.478471  0.972403 -0.031832  ...  0.013994 -0.002396  0.022853   \n",
       "1   -0.515238  0.692389  1.501606  ... -0.017403  0.061122 -0.006753   \n",
       "2   -0.035867  0.815855 -0.909473  ...  0.044420 -0.040892 -0.029098   \n",
       "3   -0.344235 -0.910867 -0.629517  ... -0.000275  0.041640  0.020649   \n",
       "4   -0.251152  0.179023 -0.233104  ... -0.009806  0.054719  0.001238   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "923 -0.392796  0.755732 -0.584050  ... -0.037846  0.077444 -0.044552   \n",
       "924  0.801522  0.690113  0.953008  ...  0.012789 -0.093530  0.013706   \n",
       "925 -0.072048  0.664386 -0.837668  ... -0.008154 -0.005854 -0.005952   \n",
       "926 -0.184308  0.461063 -0.002047  ...  0.057527  0.024187 -0.017715   \n",
       "927  0.033859  0.004444 -0.687583  ...  0.010228 -0.031783  0.075114   \n",
       "\n",
       "           94        95        96        97        98        99  target  \n",
       "0    0.010722  0.000775 -0.000213  0.009859 -0.063037 -0.024770       2  \n",
       "1    0.011152  0.000977  0.045316  0.034835 -0.036043  0.003199       2  \n",
       "2    0.121536 -0.046317 -0.019270 -0.022017  0.045859  0.007546       2  \n",
       "3   -0.052982 -0.040261 -0.015403 -0.021176  0.022025 -0.034439       2  \n",
       "4    0.032091  0.015511 -0.004524  0.043634  0.002376 -0.028378       2  \n",
       "..        ...       ...       ...       ...       ...       ...     ...  \n",
       "923 -0.032070  0.007507 -0.026487 -0.051556 -0.004306  0.010902       3  \n",
       "924 -0.029583 -0.009552  0.057532 -0.007173 -0.068559 -0.016400       3  \n",
       "925  0.006260 -0.021991  0.067267  0.021478 -0.003742 -0.010129       3  \n",
       "926 -0.080232 -0.005021  0.033426 -0.057247  0.027725  0.062772       3  \n",
       "927 -0.062239  0.010940 -0.072666  0.006255 -0.057147 -0.023021       3  \n",
       "\n",
       "[928 rows x 101 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#just for testing\n",
    "# Now Perform Dimensionality reduction (PCA) on that Dataframe and check\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#do PCA and choose componeents as 100\n",
    "pca = PCA(n_components=100)\n",
    "x_pca = pca.fit_transform(test_final.iloc[:,0:-1])\n",
    "x_pca = pd.DataFrame(x_pca)\n",
    "\n",
    "# Calculate the variance explained by priciple components\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "print('Variance of each component:', pca.explained_variance_ratio_)\n",
    "print('\\n Total Variance Explained:', round(sum(list(pca.explained_variance_ratio_))*100, 2))\n",
    "\n",
    "#store the new pca generated dimensions in a dataframe\n",
    "pca_df = pd.DataFrame(data = x_pca)\n",
    "target = pd.Series(test_final['target'], name='target')\n",
    "result_df = pd.concat([pca_df, target], axis=1)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "id": "ErMVKLMrKBiJ",
    "outputId": "f7ccdbdd-773b-4a0e-d7de-d467d120a211"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.018578</td>\n",
       "      <td>1.148263</td>\n",
       "      <td>-0.589582</td>\n",
       "      <td>0.193617</td>\n",
       "      <td>0.047950</td>\n",
       "      <td>-0.309400</td>\n",
       "      <td>-0.161566</td>\n",
       "      <td>0.478471</td>\n",
       "      <td>0.972403</td>\n",
       "      <td>-0.031832</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013994</td>\n",
       "      <td>-0.002396</td>\n",
       "      <td>0.022853</td>\n",
       "      <td>0.010722</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>-0.000213</td>\n",
       "      <td>0.009859</td>\n",
       "      <td>-0.063037</td>\n",
       "      <td>-0.024770</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.098692</td>\n",
       "      <td>0.289832</td>\n",
       "      <td>-1.766388</td>\n",
       "      <td>1.076165</td>\n",
       "      <td>-0.261201</td>\n",
       "      <td>-0.820446</td>\n",
       "      <td>-0.474188</td>\n",
       "      <td>-0.515238</td>\n",
       "      <td>0.692389</td>\n",
       "      <td>1.501606</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.017403</td>\n",
       "      <td>0.061122</td>\n",
       "      <td>-0.006753</td>\n",
       "      <td>0.011152</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.045316</td>\n",
       "      <td>0.034835</td>\n",
       "      <td>-0.036043</td>\n",
       "      <td>0.003199</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.275021</td>\n",
       "      <td>-0.451289</td>\n",
       "      <td>0.106750</td>\n",
       "      <td>-0.426415</td>\n",
       "      <td>0.066133</td>\n",
       "      <td>0.692474</td>\n",
       "      <td>0.634894</td>\n",
       "      <td>-0.035867</td>\n",
       "      <td>0.815855</td>\n",
       "      <td>-0.909473</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044420</td>\n",
       "      <td>-0.040892</td>\n",
       "      <td>-0.029098</td>\n",
       "      <td>0.121536</td>\n",
       "      <td>-0.046317</td>\n",
       "      <td>-0.019270</td>\n",
       "      <td>-0.022017</td>\n",
       "      <td>0.045859</td>\n",
       "      <td>0.007546</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.517085</td>\n",
       "      <td>1.662693</td>\n",
       "      <td>-1.021167</td>\n",
       "      <td>0.804267</td>\n",
       "      <td>-0.281985</td>\n",
       "      <td>0.518180</td>\n",
       "      <td>0.355748</td>\n",
       "      <td>-0.344235</td>\n",
       "      <td>-0.910867</td>\n",
       "      <td>-0.629517</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000275</td>\n",
       "      <td>0.041640</td>\n",
       "      <td>0.020649</td>\n",
       "      <td>-0.052982</td>\n",
       "      <td>-0.040261</td>\n",
       "      <td>-0.015403</td>\n",
       "      <td>-0.021176</td>\n",
       "      <td>0.022025</td>\n",
       "      <td>-0.034439</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.152840</td>\n",
       "      <td>-1.046283</td>\n",
       "      <td>0.351278</td>\n",
       "      <td>1.100381</td>\n",
       "      <td>-1.613642</td>\n",
       "      <td>1.484188</td>\n",
       "      <td>-0.113277</td>\n",
       "      <td>-0.251152</td>\n",
       "      <td>0.179023</td>\n",
       "      <td>-0.233104</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009806</td>\n",
       "      <td>0.054719</td>\n",
       "      <td>0.001238</td>\n",
       "      <td>0.032091</td>\n",
       "      <td>0.015511</td>\n",
       "      <td>-0.004524</td>\n",
       "      <td>0.043634</td>\n",
       "      <td>0.002376</td>\n",
       "      <td>-0.028378</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>-1.321884</td>\n",
       "      <td>2.153021</td>\n",
       "      <td>0.788596</td>\n",
       "      <td>-1.304253</td>\n",
       "      <td>0.458186</td>\n",
       "      <td>-0.859346</td>\n",
       "      <td>-0.069127</td>\n",
       "      <td>-0.392796</td>\n",
       "      <td>0.755732</td>\n",
       "      <td>-0.584050</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.037846</td>\n",
       "      <td>0.077444</td>\n",
       "      <td>-0.044552</td>\n",
       "      <td>-0.032070</td>\n",
       "      <td>0.007507</td>\n",
       "      <td>-0.026487</td>\n",
       "      <td>-0.051556</td>\n",
       "      <td>-0.004306</td>\n",
       "      <td>0.010902</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>-0.867163</td>\n",
       "      <td>-0.040504</td>\n",
       "      <td>0.940680</td>\n",
       "      <td>0.302648</td>\n",
       "      <td>-0.469672</td>\n",
       "      <td>-0.368255</td>\n",
       "      <td>1.065579</td>\n",
       "      <td>0.801522</td>\n",
       "      <td>0.690113</td>\n",
       "      <td>0.953008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012789</td>\n",
       "      <td>-0.093530</td>\n",
       "      <td>0.013706</td>\n",
       "      <td>-0.029583</td>\n",
       "      <td>-0.009552</td>\n",
       "      <td>0.057532</td>\n",
       "      <td>-0.007173</td>\n",
       "      <td>-0.068559</td>\n",
       "      <td>-0.016400</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>3.753012</td>\n",
       "      <td>0.841636</td>\n",
       "      <td>-0.317393</td>\n",
       "      <td>-0.296117</td>\n",
       "      <td>0.593769</td>\n",
       "      <td>-0.255474</td>\n",
       "      <td>-0.057091</td>\n",
       "      <td>-0.072048</td>\n",
       "      <td>0.664386</td>\n",
       "      <td>-0.837668</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008154</td>\n",
       "      <td>-0.005854</td>\n",
       "      <td>-0.005952</td>\n",
       "      <td>0.006260</td>\n",
       "      <td>-0.021991</td>\n",
       "      <td>0.067267</td>\n",
       "      <td>0.021478</td>\n",
       "      <td>-0.003742</td>\n",
       "      <td>-0.010129</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926</th>\n",
       "      <td>0.603083</td>\n",
       "      <td>0.126259</td>\n",
       "      <td>0.003433</td>\n",
       "      <td>0.283612</td>\n",
       "      <td>0.169559</td>\n",
       "      <td>-0.156326</td>\n",
       "      <td>-0.068399</td>\n",
       "      <td>-0.184308</td>\n",
       "      <td>0.461063</td>\n",
       "      <td>-0.002047</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057527</td>\n",
       "      <td>0.024187</td>\n",
       "      <td>-0.017715</td>\n",
       "      <td>-0.080232</td>\n",
       "      <td>-0.005021</td>\n",
       "      <td>0.033426</td>\n",
       "      <td>-0.057247</td>\n",
       "      <td>0.027725</td>\n",
       "      <td>0.062772</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927</th>\n",
       "      <td>-1.452945</td>\n",
       "      <td>1.233599</td>\n",
       "      <td>0.439472</td>\n",
       "      <td>0.278517</td>\n",
       "      <td>0.165928</td>\n",
       "      <td>-0.171830</td>\n",
       "      <td>-0.075000</td>\n",
       "      <td>0.033859</td>\n",
       "      <td>0.004444</td>\n",
       "      <td>-0.687583</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010228</td>\n",
       "      <td>-0.031783</td>\n",
       "      <td>0.075114</td>\n",
       "      <td>-0.062239</td>\n",
       "      <td>0.010940</td>\n",
       "      <td>-0.072666</td>\n",
       "      <td>0.006255</td>\n",
       "      <td>-0.057147</td>\n",
       "      <td>-0.023021</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>928 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "0    1.018578  1.148263 -0.589582  0.193617  0.047950 -0.309400 -0.161566   \n",
       "1   -1.098692  0.289832 -1.766388  1.076165 -0.261201 -0.820446 -0.474188   \n",
       "2    0.275021 -0.451289  0.106750 -0.426415  0.066133  0.692474  0.634894   \n",
       "3   -1.517085  1.662693 -1.021167  0.804267 -0.281985  0.518180  0.355748   \n",
       "4   -0.152840 -1.046283  0.351278  1.100381 -1.613642  1.484188 -0.113277   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "923 -1.321884  2.153021  0.788596 -1.304253  0.458186 -0.859346 -0.069127   \n",
       "924 -0.867163 -0.040504  0.940680  0.302648 -0.469672 -0.368255  1.065579   \n",
       "925  3.753012  0.841636 -0.317393 -0.296117  0.593769 -0.255474 -0.057091   \n",
       "926  0.603083  0.126259  0.003433  0.283612  0.169559 -0.156326 -0.068399   \n",
       "927 -1.452945  1.233599  0.439472  0.278517  0.165928 -0.171830 -0.075000   \n",
       "\n",
       "            7         8         9  ...        91        92        93  \\\n",
       "0    0.478471  0.972403 -0.031832  ...  0.013994 -0.002396  0.022853   \n",
       "1   -0.515238  0.692389  1.501606  ... -0.017403  0.061122 -0.006753   \n",
       "2   -0.035867  0.815855 -0.909473  ...  0.044420 -0.040892 -0.029098   \n",
       "3   -0.344235 -0.910867 -0.629517  ... -0.000275  0.041640  0.020649   \n",
       "4   -0.251152  0.179023 -0.233104  ... -0.009806  0.054719  0.001238   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "923 -0.392796  0.755732 -0.584050  ... -0.037846  0.077444 -0.044552   \n",
       "924  0.801522  0.690113  0.953008  ...  0.012789 -0.093530  0.013706   \n",
       "925 -0.072048  0.664386 -0.837668  ... -0.008154 -0.005854 -0.005952   \n",
       "926 -0.184308  0.461063 -0.002047  ...  0.057527  0.024187 -0.017715   \n",
       "927  0.033859  0.004444 -0.687583  ...  0.010228 -0.031783  0.075114   \n",
       "\n",
       "           94        95        96        97        98        99  target  \n",
       "0    0.010722  0.000775 -0.000213  0.009859 -0.063037 -0.024770       2  \n",
       "1    0.011152  0.000977  0.045316  0.034835 -0.036043  0.003199       2  \n",
       "2    0.121536 -0.046317 -0.019270 -0.022017  0.045859  0.007546       2  \n",
       "3   -0.052982 -0.040261 -0.015403 -0.021176  0.022025 -0.034439       2  \n",
       "4    0.032091  0.015511 -0.004524  0.043634  0.002376 -0.028378       2  \n",
       "..        ...       ...       ...       ...       ...       ...     ...  \n",
       "923 -0.032070  0.007507 -0.026487 -0.051556 -0.004306  0.010902       3  \n",
       "924 -0.029583 -0.009552  0.057532 -0.007173 -0.068559 -0.016400       3  \n",
       "925  0.006260 -0.021991  0.067267  0.021478 -0.003742 -0.010129       3  \n",
       "926 -0.080232 -0.005021  0.033426 -0.057247  0.027725  0.062772       3  \n",
       "927 -0.062239  0.010940 -0.072666  0.006255 -0.057147 -0.023021       3  \n",
       "\n",
       "[928 rows x 101 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8UOI2lH6iOIY"
   },
   "source": [
    "#### **TRYING DIFFERENT ML MODELS ON A SINGLE LEAD(EX : 1) POST DIMENSIONALITY REDUCTION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rE3l8LNQizo0"
   },
   "source": [
    "##### **KNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Me4jqP4Zih_I",
    "outputId": "83dde896-7166-44cb-b88f-725eaedb9d34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.782258064516129\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.63      0.73       105\n",
      "           1       0.91      0.91      0.91        94\n",
      "           2       0.72      0.88      0.79       112\n",
      "           3       0.63      0.67      0.65        61\n",
      "\n",
      "    accuracy                           0.78       372\n",
      "   macro avg       0.78      0.77      0.77       372\n",
      "weighted avg       0.80      0.78      0.78       372\n",
      "\n",
      "Tuned Model Parameters: {'knn__n_neighbors': 1}\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules for ML model\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Setup the pipeline steps:\n",
    "steps = [('knn', KNeighborsClassifier())]\n",
    "        \n",
    "# Create the pipeline: pipeline \n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# have paased less range value of hyperparamter since i'm using free tier version of google colab.\n",
    "k_range = list(range(1, 9))\n",
    "parameters = dict(knn__n_neighbors=k_range)\n",
    "\n",
    "#input\n",
    "X = result_df.iloc[:,0:-1]\n",
    "\n",
    "#target\n",
    "y=result_df.iloc[:,-1]\n",
    "\n",
    "# Create train and test sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.4,random_state=42)\n",
    "\n",
    "#increasing cv score takes lot of time in gooogle colab, so kept it just 2.\n",
    "cv = GridSearchCV(pipeline,parameters,cv=2)\n",
    "\n",
    "cv.fit(X_train,y_train)\n",
    "\n",
    "# Predict the labels of the test set: y_pred\n",
    "y_pred = cv.predict(X_test)\n",
    "\n",
    "Knn_Accuracy = cv.score(X_test, y_test)\n",
    "\n",
    "# Compute and print metrics\n",
    "print(\"Accuracy: {}\".format(Knn_Accuracy))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Tuned Model Parameters: {}\".format(cv.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VDkFlXChia-8"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RKtMbgXLifbX"
   },
   "source": [
    "##### **LOGISTIC REGRESSION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "BbGt_eOVkJq6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vijay\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\vijay\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\vijay\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Setup the pipeline steps:\n",
    "steps = [('lr', LogisticRegression())]\n",
    "        \n",
    "# Create the pipeline: pipeline \n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "#input\n",
    "X = result_df.iloc[:,0:-1]\n",
    "\n",
    "#target\n",
    "y=result_df.iloc[:,-1]\n",
    "\n",
    "#parameters for gridsearchcv if we increase range of entries from 5 to higher value, we can get greater accurange\n",
    "c_space = np.logspace(-4, 4, 3)\n",
    "parameters = {'lr__C': c_space,'lr__penalty': ['l2']} \n",
    "\n",
    "# Create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.4,random_state=42)\n",
    "\n",
    "#call GridSearchCV and set crossvalscore to 2\n",
    "cv = GridSearchCV(pipeline,parameters,cv=2)\n",
    "\n",
    "cv.fit(X_train,y_train)\n",
    "\n",
    "# Predict the labels of the test set: y_pred\n",
    "y_pred = cv.predict(X_test)\n",
    "LR_Accuracy = cv.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3k9i12eGunX1",
    "outputId": "21ce6e7f-653e-4710-d585-ff445736e08b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5510752688172043\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.34      0.36       105\n",
      "           1       0.73      0.91      0.81        94\n",
      "           2       0.58      0.60      0.59       112\n",
      "           3       0.37      0.26      0.31        61\n",
      "\n",
      "    accuracy                           0.55       372\n",
      "   macro avg       0.51      0.53      0.52       372\n",
      "weighted avg       0.53      0.55      0.53       372\n",
      "\n",
      "Tuned Model Parameters: {'lr__C': 10000.0, 'lr__penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "# Compute and print metrics\n",
    "print(\"Accuracy: {}\".format(LR_Accuracy))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Tuned Model Parameters: {}\".format(cv.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_W0sQQDwiisE"
   },
   "source": [
    "##### **SVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "94xczE1iurC9",
    "outputId": "d5530fa8-865f-4ec0-80c9-f89940866c43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8225806451612904\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      1.00      0.74        93\n",
      "           1       1.00      1.00      1.00        99\n",
      "           2       1.00      0.61      0.76       117\n",
      "           3       1.00      0.68      0.81        63\n",
      "\n",
      "    accuracy                           0.82       372\n",
      "   macro avg       0.90      0.82      0.83       372\n",
      "weighted avg       0.90      0.82      0.83       372\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules for ML model\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Setup the pipeline\n",
    "steps = [('SVM', SVC())]\n",
    "\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "#input\n",
    "X = result_df.iloc[:,0:-1]\n",
    "\n",
    "#target\n",
    "y=result_df.iloc[:,-1]\n",
    "\n",
    "# Specify the hyperparameter space, if we increase the penalty(c) and gamma value the accurancy can be increased.\n",
    "#since it takes lots of time in google colab provided only a single value\n",
    "parameters = {'SVM__C':[10],'SVM__gamma':[1]}\n",
    "\n",
    "# Create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.4,random_state=21)\n",
    "\n",
    "cv = GridSearchCV(pipeline,parameters,cv=3)\n",
    "cv.fit(X_train,y_train)\n",
    "\n",
    "y_pred = cv.predict(X_test)\n",
    "SVM_Accuracy = cv.score(X_test, y_test)\n",
    "\n",
    "# Compute and print metrics\n",
    "SVM_Accuracy=cv.score(X_test, y_test)\n",
    "\n",
    "print(\"Accuracy: {}\".format(SVM_Accuracy))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y1GNZtOUi6PP"
   },
   "source": [
    "#### **NOW COMBINING ALL 12 LEADS INTO A SINGLE CSV FILE AND THEN PERFROM MODEL ANALYSIS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "id": "huNy0hsWSkr5",
    "outputId": "ae4fc5c3-8b46-4d0a-d075-c2c64559680c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>3051</th>\n",
       "      <th>3052</th>\n",
       "      <th>3053</th>\n",
       "      <th>3054</th>\n",
       "      <th>3055</th>\n",
       "      <th>3056</th>\n",
       "      <th>3057</th>\n",
       "      <th>3058</th>\n",
       "      <th>3059</th>\n",
       "      <th>3060</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.728449</td>\n",
       "      <td>0.680755</td>\n",
       "      <td>0.619010</td>\n",
       "      <td>0.645367</td>\n",
       "      <td>0.681570</td>\n",
       "      <td>0.732488</td>\n",
       "      <td>0.758448</td>\n",
       "      <td>0.750660</td>\n",
       "      <td>0.728282</td>\n",
       "      <td>0.707928</td>\n",
       "      <td>...</td>\n",
       "      <td>0.864067</td>\n",
       "      <td>0.849256</td>\n",
       "      <td>0.854949</td>\n",
       "      <td>0.861380</td>\n",
       "      <td>0.875514</td>\n",
       "      <td>0.868763</td>\n",
       "      <td>0.847450</td>\n",
       "      <td>0.805689</td>\n",
       "      <td>0.751761</td>\n",
       "      <td>0.702102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.957972</td>\n",
       "      <td>0.950695</td>\n",
       "      <td>0.941024</td>\n",
       "      <td>0.930501</td>\n",
       "      <td>0.913601</td>\n",
       "      <td>0.892244</td>\n",
       "      <td>0.868016</td>\n",
       "      <td>0.855127</td>\n",
       "      <td>0.835307</td>\n",
       "      <td>0.798640</td>\n",
       "      <td>...</td>\n",
       "      <td>0.925865</td>\n",
       "      <td>0.928285</td>\n",
       "      <td>0.946033</td>\n",
       "      <td>0.947274</td>\n",
       "      <td>0.946394</td>\n",
       "      <td>0.936536</td>\n",
       "      <td>0.920869</td>\n",
       "      <td>0.910320</td>\n",
       "      <td>0.905436</td>\n",
       "      <td>0.876942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.611084</td>\n",
       "      <td>0.661575</td>\n",
       "      <td>0.695790</td>\n",
       "      <td>0.741113</td>\n",
       "      <td>0.716666</td>\n",
       "      <td>0.595794</td>\n",
       "      <td>0.425022</td>\n",
       "      <td>0.286457</td>\n",
       "      <td>0.425022</td>\n",
       "      <td>0.611384</td>\n",
       "      <td>...</td>\n",
       "      <td>0.170137</td>\n",
       "      <td>0.166206</td>\n",
       "      <td>0.207633</td>\n",
       "      <td>0.258184</td>\n",
       "      <td>0.286993</td>\n",
       "      <td>0.304742</td>\n",
       "      <td>0.325659</td>\n",
       "      <td>0.361189</td>\n",
       "      <td>0.451946</td>\n",
       "      <td>0.543373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.839213</td>\n",
       "      <td>0.861690</td>\n",
       "      <td>0.866457</td>\n",
       "      <td>0.865756</td>\n",
       "      <td>0.855027</td>\n",
       "      <td>0.855606</td>\n",
       "      <td>0.845561</td>\n",
       "      <td>0.843187</td>\n",
       "      <td>0.846784</td>\n",
       "      <td>0.824438</td>\n",
       "      <td>...</td>\n",
       "      <td>0.880043</td>\n",
       "      <td>0.883833</td>\n",
       "      <td>0.870995</td>\n",
       "      <td>0.861323</td>\n",
       "      <td>0.864892</td>\n",
       "      <td>0.863552</td>\n",
       "      <td>0.839506</td>\n",
       "      <td>0.805486</td>\n",
       "      <td>0.801828</td>\n",
       "      <td>0.826618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.917753</td>\n",
       "      <td>0.924369</td>\n",
       "      <td>0.873765</td>\n",
       "      <td>0.791381</td>\n",
       "      <td>0.699513</td>\n",
       "      <td>0.604927</td>\n",
       "      <td>0.500312</td>\n",
       "      <td>0.446012</td>\n",
       "      <td>0.528910</td>\n",
       "      <td>0.634068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.400774</td>\n",
       "      <td>0.380920</td>\n",
       "      <td>0.439510</td>\n",
       "      <td>0.505257</td>\n",
       "      <td>0.561538</td>\n",
       "      <td>0.577997</td>\n",
       "      <td>0.566082</td>\n",
       "      <td>0.547642</td>\n",
       "      <td>0.538735</td>\n",
       "      <td>0.527560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>0.874246</td>\n",
       "      <td>0.877014</td>\n",
       "      <td>0.864280</td>\n",
       "      <td>0.860505</td>\n",
       "      <td>0.871349</td>\n",
       "      <td>0.912404</td>\n",
       "      <td>0.958148</td>\n",
       "      <td>0.977826</td>\n",
       "      <td>0.956314</td>\n",
       "      <td>0.926773</td>\n",
       "      <td>...</td>\n",
       "      <td>0.785103</td>\n",
       "      <td>0.676795</td>\n",
       "      <td>0.579054</td>\n",
       "      <td>0.476613</td>\n",
       "      <td>0.458748</td>\n",
       "      <td>0.565470</td>\n",
       "      <td>0.681896</td>\n",
       "      <td>0.792646</td>\n",
       "      <td>0.871660</td>\n",
       "      <td>0.872789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>0.829815</td>\n",
       "      <td>0.832084</td>\n",
       "      <td>0.852396</td>\n",
       "      <td>0.909665</td>\n",
       "      <td>0.988242</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923323</td>\n",
       "      <td>0.821865</td>\n",
       "      <td>0.721302</td>\n",
       "      <td>0.612039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.913404</td>\n",
       "      <td>0.968015</td>\n",
       "      <td>0.992614</td>\n",
       "      <td>0.945789</td>\n",
       "      <td>0.876660</td>\n",
       "      <td>0.808906</td>\n",
       "      <td>0.741645</td>\n",
       "      <td>0.736615</td>\n",
       "      <td>0.797729</td>\n",
       "      <td>0.855637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>0.469048</td>\n",
       "      <td>0.417983</td>\n",
       "      <td>0.362322</td>\n",
       "      <td>0.351995</td>\n",
       "      <td>0.391493</td>\n",
       "      <td>0.418305</td>\n",
       "      <td>0.440135</td>\n",
       "      <td>0.444598</td>\n",
       "      <td>0.460402</td>\n",
       "      <td>0.506810</td>\n",
       "      <td>...</td>\n",
       "      <td>0.730354</td>\n",
       "      <td>0.697465</td>\n",
       "      <td>0.714527</td>\n",
       "      <td>0.745605</td>\n",
       "      <td>0.754952</td>\n",
       "      <td>0.755059</td>\n",
       "      <td>0.755059</td>\n",
       "      <td>0.755093</td>\n",
       "      <td>0.759093</td>\n",
       "      <td>0.767555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926</th>\n",
       "      <td>0.682510</td>\n",
       "      <td>0.682286</td>\n",
       "      <td>0.641051</td>\n",
       "      <td>0.620212</td>\n",
       "      <td>0.608210</td>\n",
       "      <td>0.576331</td>\n",
       "      <td>0.603596</td>\n",
       "      <td>0.645714</td>\n",
       "      <td>0.677964</td>\n",
       "      <td>0.720297</td>\n",
       "      <td>...</td>\n",
       "      <td>0.860939</td>\n",
       "      <td>0.824976</td>\n",
       "      <td>0.783459</td>\n",
       "      <td>0.761391</td>\n",
       "      <td>0.741917</td>\n",
       "      <td>0.770631</td>\n",
       "      <td>0.802701</td>\n",
       "      <td>0.821503</td>\n",
       "      <td>0.846300</td>\n",
       "      <td>0.858795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927</th>\n",
       "      <td>0.792175</td>\n",
       "      <td>0.815695</td>\n",
       "      <td>0.819518</td>\n",
       "      <td>0.820559</td>\n",
       "      <td>0.847985</td>\n",
       "      <td>0.880933</td>\n",
       "      <td>0.902061</td>\n",
       "      <td>0.878266</td>\n",
       "      <td>0.838806</td>\n",
       "      <td>0.811795</td>\n",
       "      <td>...</td>\n",
       "      <td>0.516381</td>\n",
       "      <td>0.521133</td>\n",
       "      <td>0.521142</td>\n",
       "      <td>0.521143</td>\n",
       "      <td>0.522801</td>\n",
       "      <td>0.543166</td>\n",
       "      <td>0.549073</td>\n",
       "      <td>0.564977</td>\n",
       "      <td>0.576139</td>\n",
       "      <td>0.576267</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>928 rows × 3060 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6     \\\n",
       "0    0.728449  0.680755  0.619010  0.645367  0.681570  0.732488  0.758448   \n",
       "1    0.957972  0.950695  0.941024  0.930501  0.913601  0.892244  0.868016   \n",
       "2    0.611084  0.661575  0.695790  0.741113  0.716666  0.595794  0.425022   \n",
       "3    0.839213  0.861690  0.866457  0.865756  0.855027  0.855606  0.845561   \n",
       "4    0.917753  0.924369  0.873765  0.791381  0.699513  0.604927  0.500312   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "923  0.874246  0.877014  0.864280  0.860505  0.871349  0.912404  0.958148   \n",
       "924  0.829815  0.832084  0.852396  0.909665  0.988242  1.000000  0.923323   \n",
       "925  0.469048  0.417983  0.362322  0.351995  0.391493  0.418305  0.440135   \n",
       "926  0.682510  0.682286  0.641051  0.620212  0.608210  0.576331  0.603596   \n",
       "927  0.792175  0.815695  0.819518  0.820559  0.847985  0.880933  0.902061   \n",
       "\n",
       "         7         8         9     ...      3051      3052      3053  \\\n",
       "0    0.750660  0.728282  0.707928  ...  0.864067  0.849256  0.854949   \n",
       "1    0.855127  0.835307  0.798640  ...  0.925865  0.928285  0.946033   \n",
       "2    0.286457  0.425022  0.611384  ...  0.170137  0.166206  0.207633   \n",
       "3    0.843187  0.846784  0.824438  ...  0.880043  0.883833  0.870995   \n",
       "4    0.446012  0.528910  0.634068  ...  0.400774  0.380920  0.439510   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "923  0.977826  0.956314  0.926773  ...  0.785103  0.676795  0.579054   \n",
       "924  0.821865  0.721302  0.612039  ...  0.913404  0.968015  0.992614   \n",
       "925  0.444598  0.460402  0.506810  ...  0.730354  0.697465  0.714527   \n",
       "926  0.645714  0.677964  0.720297  ...  0.860939  0.824976  0.783459   \n",
       "927  0.878266  0.838806  0.811795  ...  0.516381  0.521133  0.521142   \n",
       "\n",
       "         3054      3055      3056      3057      3058      3059      3060  \n",
       "0    0.861380  0.875514  0.868763  0.847450  0.805689  0.751761  0.702102  \n",
       "1    0.947274  0.946394  0.936536  0.920869  0.910320  0.905436  0.876942  \n",
       "2    0.258184  0.286993  0.304742  0.325659  0.361189  0.451946  0.543373  \n",
       "3    0.861323  0.864892  0.863552  0.839506  0.805486  0.801828  0.826618  \n",
       "4    0.505257  0.561538  0.577997  0.566082  0.547642  0.538735  0.527560  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "923  0.476613  0.458748  0.565470  0.681896  0.792646  0.871660  0.872789  \n",
       "924  0.945789  0.876660  0.808906  0.741645  0.736615  0.797729  0.855637  \n",
       "925  0.745605  0.754952  0.755059  0.755059  0.755093  0.759093  0.767555  \n",
       "926  0.761391  0.741917  0.770631  0.802701  0.821503  0.846300  0.858795  \n",
       "927  0.521143  0.522801  0.543166  0.549073  0.564977  0.576139  0.576267  \n",
       "\n",
       "[928 rows x 3060 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets try combining all 12 leads in a single csv\n",
    "location= '../preprocessed_1d/'\n",
    "for files in natsorted(os.listdir(location)):\n",
    "  if files.endswith(\".csv\") and not files.endswith(\"13.csv\"):\n",
    "    if files!='Combined_IDLead_1.csv':\n",
    "      df=pd.read_csv('../preprocessed_1d/{}'.format(files))\n",
    "      df.drop(columns=['Unnamed: 0'],inplace=True)\n",
    "      test_final=pd.concat([test_final,df],axis=1,ignore_index=True)\n",
    "      test_final.drop(columns=test_final.columns[-1],axis=1,inplace=True)\n",
    "\n",
    "#drop the target column\n",
    "test_final.drop(columns=[255],axis=1,inplace=True)\n",
    "test_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "OZ6_Lg0180y3"
   },
   "outputs": [],
   "source": [
    "#write the final file to csv\n",
    "test_final.to_csv('final_1D.csv',header=False,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bxWK-X-qjde2"
   },
   "source": [
    "#### **TEST DIMENSIONALITY REDUCTION EXPLAINED VARIANCE ON  THE DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "WXvZGdh5cxrL",
    "outputId": "63b7ea81-03df-43ae-b708-630b9ce6722f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance of each component: [8.04649534e-02 4.68818003e-02 3.76212504e-02 2.94708618e-02\n",
      " 2.57031130e-02 2.32574514e-02 2.14376788e-02 2.04315151e-02\n",
      " 1.94482863e-02 1.79877408e-02 1.64766264e-02 1.53241665e-02\n",
      " 1.50689862e-02 1.41398267e-02 1.36330466e-02 1.33375324e-02\n",
      " 1.26355566e-02 1.25577001e-02 1.16968257e-02 1.11671338e-02\n",
      " 1.07975552e-02 1.06183806e-02 1.03402122e-02 1.01248410e-02\n",
      " 9.73197948e-03 9.25504395e-03 9.16367637e-03 8.76267060e-03\n",
      " 8.54270112e-03 8.20665462e-03 8.07642149e-03 7.90742343e-03\n",
      " 7.54929819e-03 7.21938018e-03 7.07604659e-03 6.89135251e-03\n",
      " 6.80575532e-03 6.71875790e-03 6.38252148e-03 6.33951897e-03\n",
      " 6.10254734e-03 5.94560955e-03 5.76371295e-03 5.71788829e-03\n",
      " 5.55354810e-03 5.42316932e-03 5.35640711e-03 5.08429353e-03\n",
      " 5.03302777e-03 4.96811576e-03 4.87696491e-03 4.63686128e-03\n",
      " 4.55349933e-03 4.45390625e-03 4.31579996e-03 4.28316592e-03\n",
      " 4.17213140e-03 4.12346241e-03 4.09072049e-03 3.99349122e-03\n",
      " 3.92129459e-03 3.81982060e-03 3.78116652e-03 3.73307150e-03\n",
      " 3.68894307e-03 3.55238746e-03 3.49148625e-03 3.40490507e-03\n",
      " 3.33593814e-03 3.25467389e-03 3.20023474e-03 3.14871964e-03\n",
      " 3.09091665e-03 3.07180393e-03 3.05651457e-03 2.95447952e-03\n",
      " 2.90507083e-03 2.84618700e-03 2.80939396e-03 2.76324718e-03\n",
      " 2.71487874e-03 2.68959207e-03 2.67378836e-03 2.62085254e-03\n",
      " 2.55991613e-03 2.53614502e-03 2.47015404e-03 2.45768102e-03\n",
      " 2.41851536e-03 2.39477316e-03 2.35560704e-03 2.29236345e-03\n",
      " 2.26928539e-03 2.24965527e-03 2.22764534e-03 2.19258829e-03\n",
      " 2.14654982e-03 2.09081474e-03 2.08656961e-03 2.04315332e-03\n",
      " 2.01191187e-03 1.99715030e-03 1.98092986e-03 1.93183566e-03\n",
      " 1.90133601e-03 1.86628808e-03 1.85847904e-03 1.79040117e-03\n",
      " 1.77318190e-03 1.76278440e-03 1.73682193e-03 1.70177712e-03\n",
      " 1.69142157e-03 1.66289246e-03 1.64192361e-03 1.62455779e-03\n",
      " 1.59836820e-03 1.57166872e-03 1.56017874e-03 1.55193712e-03\n",
      " 1.52130395e-03 1.50860404e-03 1.48563216e-03 1.45667689e-03\n",
      " 1.44862677e-03 1.43014707e-03 1.42443426e-03 1.39341888e-03\n",
      " 1.38941740e-03 1.38032166e-03 1.35292505e-03 1.33403513e-03\n",
      " 1.33300728e-03 1.31774024e-03 1.29238722e-03 1.24574072e-03\n",
      " 1.23408862e-03 1.21598644e-03 1.20568485e-03 1.19391143e-03\n",
      " 1.18690274e-03 1.16630751e-03 1.16159095e-03 1.14539199e-03\n",
      " 1.13634359e-03 1.11858663e-03 1.10460060e-03 1.08515359e-03\n",
      " 1.07679695e-03 1.06488284e-03 1.05861426e-03 1.04012565e-03\n",
      " 1.03222232e-03 1.02519590e-03 1.01169941e-03 9.96444257e-04\n",
      " 9.76134514e-04 9.61104386e-04 9.57134099e-04 9.48294848e-04\n",
      " 9.35386446e-04 9.29858628e-04 9.24107282e-04 9.20229599e-04\n",
      " 9.00136970e-04 8.84392791e-04 8.60041244e-04 8.58222437e-04\n",
      " 8.39586154e-04 8.34156616e-04 8.24745137e-04 8.19630377e-04\n",
      " 8.11755902e-04 8.09589697e-04 7.93351930e-04 7.83229226e-04\n",
      " 7.69323633e-04 7.62916710e-04 7.61217310e-04 7.49412461e-04\n",
      " 7.41978508e-04 7.32319449e-04 7.28386324e-04 7.15766463e-04\n",
      " 7.00416470e-04 6.92792928e-04 6.87860571e-04 6.77118996e-04\n",
      " 6.69195650e-04 6.62776506e-04 6.52787237e-04 6.41350808e-04\n",
      " 6.31671343e-04 6.25941688e-04 6.20986817e-04 6.12964320e-04\n",
      " 6.06757241e-04 6.00414979e-04 5.90442751e-04 5.85447566e-04\n",
      " 5.82053388e-04 5.72736727e-04 5.64768427e-04 5.62060875e-04\n",
      " 5.53942338e-04 5.47413376e-04 5.43815848e-04 5.39018247e-04\n",
      " 5.31538797e-04 5.21422265e-04 5.16620308e-04 5.13730677e-04\n",
      " 5.08883049e-04 5.04308685e-04 4.96238365e-04 4.91958416e-04\n",
      " 4.80055673e-04 4.74422583e-04 4.69414332e-04 4.65649130e-04\n",
      " 4.62052066e-04 4.58664174e-04 4.49131975e-04 4.46512859e-04\n",
      " 4.45747676e-04 4.36928354e-04 4.30056928e-04 4.24233411e-04\n",
      " 4.21656146e-04 4.20467958e-04 4.16760270e-04 4.15888837e-04\n",
      " 4.07286799e-04 4.03273136e-04 3.97207457e-04 3.91816956e-04\n",
      " 3.87932379e-04 3.83350761e-04 3.82143411e-04 3.79404869e-04\n",
      " 3.72582506e-04 3.63610221e-04 3.59554003e-04 3.56530332e-04\n",
      " 3.53235193e-04 3.50500785e-04 3.47735142e-04 3.42040357e-04\n",
      " 3.38310295e-04 3.37306487e-04 3.35511671e-04 3.28894986e-04\n",
      " 3.28218976e-04 3.24306967e-04 3.20401231e-04 3.14253008e-04\n",
      " 3.10208387e-04 3.06218858e-04 3.04335173e-04 3.01021071e-04\n",
      " 2.97566462e-04 2.95304843e-04 2.91254910e-04 2.89125020e-04\n",
      " 2.82693311e-04 2.76521036e-04 2.75269038e-04 2.74379690e-04\n",
      " 2.68266940e-04 2.68061606e-04 2.66099930e-04 2.64421116e-04\n",
      " 2.63834024e-04 2.57431346e-04 2.54138636e-04 2.51605526e-04\n",
      " 2.46905186e-04 2.43635580e-04 2.42082687e-04 2.40633797e-04\n",
      " 2.37784328e-04 2.37571728e-04 2.35490219e-04 2.31228801e-04\n",
      " 2.29666776e-04 2.22983273e-04 2.22297122e-04 2.19367155e-04\n",
      " 2.18709669e-04 2.16024068e-04 2.12842086e-04 2.10474615e-04\n",
      " 2.08847346e-04 2.05998013e-04 2.04966474e-04 2.02934862e-04\n",
      " 1.99807447e-04 1.96639314e-04 1.94997107e-04 1.93136553e-04\n",
      " 1.91195947e-04 1.89710889e-04 1.88308515e-04 1.84369274e-04\n",
      " 1.82567394e-04 1.80379139e-04 1.76608318e-04 1.75641699e-04\n",
      " 1.72429919e-04 1.71789436e-04 1.71183502e-04 1.69724705e-04\n",
      " 1.67456727e-04 1.65838166e-04 1.63726081e-04 1.59524241e-04\n",
      " 1.59160349e-04 1.57750156e-04 1.56407674e-04 1.54528794e-04\n",
      " 1.51822820e-04 1.51097399e-04 1.49351983e-04 1.45432419e-04\n",
      " 1.42825984e-04 1.41656486e-04 1.38463697e-04 1.36996158e-04\n",
      " 1.36519478e-04 1.35598088e-04 1.33878676e-04 1.32421437e-04\n",
      " 1.31172049e-04 1.30722197e-04 1.29582225e-04 1.25887509e-04\n",
      " 1.25079285e-04 1.23562965e-04 1.22231501e-04 1.21556094e-04\n",
      " 1.20752737e-04 1.18329389e-04 1.17293396e-04 1.15517402e-04\n",
      " 1.14236611e-04 1.13110698e-04 1.11447907e-04 1.10112769e-04\n",
      " 1.08715526e-04 1.08215872e-04 1.07081545e-04 1.04946674e-04\n",
      " 1.04071383e-04 1.02700442e-04 1.01972611e-04 1.00968667e-04\n",
      " 9.90751196e-05 9.81206280e-05 9.72010099e-05 9.52259819e-05\n",
      " 9.46258875e-05 9.40951582e-05 9.27353525e-05 9.06071063e-05\n",
      " 8.90610711e-05 8.77654253e-05 8.67630444e-05 8.57405354e-05\n",
      " 8.48020864e-05 8.37946894e-05 8.30186135e-05 8.17345470e-05\n",
      " 8.05672472e-05 8.03200677e-05 7.97322275e-05 7.85408244e-05\n",
      " 7.76167712e-05 7.65078769e-05 7.56721119e-05 7.53041802e-05\n",
      " 7.39611238e-05 7.30238410e-05 7.17782477e-05 7.09938870e-05\n",
      " 6.99898882e-05 6.89459743e-05 6.77645541e-05 6.70548894e-05\n",
      " 6.66587816e-05 6.45448753e-05 6.34935113e-05 6.26165749e-05\n",
      " 6.20487903e-05 6.04548938e-05 6.01842192e-05 5.94287163e-05\n",
      " 5.89335680e-05 5.70976242e-05 5.69509543e-05 5.60109381e-05]\n",
      "\n",
      " Total Variance Explained: 99.7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>391</th>\n",
       "      <th>392</th>\n",
       "      <th>393</th>\n",
       "      <th>394</th>\n",
       "      <th>395</th>\n",
       "      <th>396</th>\n",
       "      <th>397</th>\n",
       "      <th>398</th>\n",
       "      <th>399</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.457260</td>\n",
       "      <td>0.433402</td>\n",
       "      <td>-0.951313</td>\n",
       "      <td>-0.118767</td>\n",
       "      <td>0.372034</td>\n",
       "      <td>1.917341</td>\n",
       "      <td>0.503248</td>\n",
       "      <td>-0.996278</td>\n",
       "      <td>-2.306744</td>\n",
       "      <td>-1.294890</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001274</td>\n",
       "      <td>0.137547</td>\n",
       "      <td>-0.054768</td>\n",
       "      <td>-0.185737</td>\n",
       "      <td>0.108261</td>\n",
       "      <td>-0.044993</td>\n",
       "      <td>-0.069940</td>\n",
       "      <td>-0.033743</td>\n",
       "      <td>0.068947</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.482871</td>\n",
       "      <td>-3.923833</td>\n",
       "      <td>1.001672</td>\n",
       "      <td>1.151786</td>\n",
       "      <td>-0.105634</td>\n",
       "      <td>0.935446</td>\n",
       "      <td>0.061538</td>\n",
       "      <td>1.340459</td>\n",
       "      <td>-4.633610</td>\n",
       "      <td>-0.483502</td>\n",
       "      <td>...</td>\n",
       "      <td>0.065975</td>\n",
       "      <td>0.004736</td>\n",
       "      <td>0.199824</td>\n",
       "      <td>-0.007765</td>\n",
       "      <td>-0.104836</td>\n",
       "      <td>-0.119618</td>\n",
       "      <td>-0.078848</td>\n",
       "      <td>0.025403</td>\n",
       "      <td>-0.049254</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.524849</td>\n",
       "      <td>-0.064380</td>\n",
       "      <td>-2.177830</td>\n",
       "      <td>-0.744632</td>\n",
       "      <td>-3.160053</td>\n",
       "      <td>-2.079289</td>\n",
       "      <td>1.991373</td>\n",
       "      <td>2.629458</td>\n",
       "      <td>1.251710</td>\n",
       "      <td>1.656291</td>\n",
       "      <td>...</td>\n",
       "      <td>0.125212</td>\n",
       "      <td>-0.007987</td>\n",
       "      <td>0.003295</td>\n",
       "      <td>-0.059482</td>\n",
       "      <td>0.083645</td>\n",
       "      <td>0.043782</td>\n",
       "      <td>-0.047199</td>\n",
       "      <td>-0.097477</td>\n",
       "      <td>-0.056039</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-4.626337</td>\n",
       "      <td>-0.901579</td>\n",
       "      <td>-1.351431</td>\n",
       "      <td>-1.767117</td>\n",
       "      <td>0.870166</td>\n",
       "      <td>2.210025</td>\n",
       "      <td>3.710694</td>\n",
       "      <td>-1.478115</td>\n",
       "      <td>-2.406238</td>\n",
       "      <td>-3.754295</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004587</td>\n",
       "      <td>-0.201108</td>\n",
       "      <td>0.008220</td>\n",
       "      <td>0.045321</td>\n",
       "      <td>0.022103</td>\n",
       "      <td>-0.052815</td>\n",
       "      <td>-0.030927</td>\n",
       "      <td>-0.099674</td>\n",
       "      <td>-0.110996</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-3.863027</td>\n",
       "      <td>-2.805259</td>\n",
       "      <td>-0.428457</td>\n",
       "      <td>-0.482452</td>\n",
       "      <td>0.231498</td>\n",
       "      <td>1.977056</td>\n",
       "      <td>1.605510</td>\n",
       "      <td>-0.603866</td>\n",
       "      <td>-1.312170</td>\n",
       "      <td>3.553396</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.082930</td>\n",
       "      <td>0.000911</td>\n",
       "      <td>-0.015292</td>\n",
       "      <td>0.048979</td>\n",
       "      <td>0.103508</td>\n",
       "      <td>-0.232134</td>\n",
       "      <td>0.042196</td>\n",
       "      <td>0.165128</td>\n",
       "      <td>-0.068009</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>923</th>\n",
       "      <td>-3.365774</td>\n",
       "      <td>-2.530997</td>\n",
       "      <td>-0.781861</td>\n",
       "      <td>0.567793</td>\n",
       "      <td>1.960806</td>\n",
       "      <td>2.319130</td>\n",
       "      <td>3.632739</td>\n",
       "      <td>2.072028</td>\n",
       "      <td>0.318600</td>\n",
       "      <td>1.423024</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009309</td>\n",
       "      <td>-0.060514</td>\n",
       "      <td>0.149956</td>\n",
       "      <td>0.077576</td>\n",
       "      <td>-0.041027</td>\n",
       "      <td>-0.025089</td>\n",
       "      <td>-0.085983</td>\n",
       "      <td>-0.044497</td>\n",
       "      <td>0.243577</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>-6.340754</td>\n",
       "      <td>0.634164</td>\n",
       "      <td>-3.259888</td>\n",
       "      <td>2.894988</td>\n",
       "      <td>2.625952</td>\n",
       "      <td>-1.435685</td>\n",
       "      <td>1.570297</td>\n",
       "      <td>0.456509</td>\n",
       "      <td>-0.855480</td>\n",
       "      <td>0.523852</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200018</td>\n",
       "      <td>0.221275</td>\n",
       "      <td>0.268479</td>\n",
       "      <td>0.089746</td>\n",
       "      <td>-0.050179</td>\n",
       "      <td>-0.012088</td>\n",
       "      <td>0.173675</td>\n",
       "      <td>0.037648</td>\n",
       "      <td>-0.061817</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>-2.106097</td>\n",
       "      <td>4.489429</td>\n",
       "      <td>4.740686</td>\n",
       "      <td>2.521037</td>\n",
       "      <td>1.810682</td>\n",
       "      <td>1.252919</td>\n",
       "      <td>-0.925497</td>\n",
       "      <td>2.413092</td>\n",
       "      <td>-0.426080</td>\n",
       "      <td>-2.335654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039288</td>\n",
       "      <td>-0.030993</td>\n",
       "      <td>-0.061379</td>\n",
       "      <td>0.066061</td>\n",
       "      <td>-0.091756</td>\n",
       "      <td>0.094224</td>\n",
       "      <td>0.183882</td>\n",
       "      <td>0.008111</td>\n",
       "      <td>-0.095113</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926</th>\n",
       "      <td>-1.091978</td>\n",
       "      <td>7.063249</td>\n",
       "      <td>2.692234</td>\n",
       "      <td>-0.543705</td>\n",
       "      <td>-0.443376</td>\n",
       "      <td>1.028903</td>\n",
       "      <td>-1.749000</td>\n",
       "      <td>0.469497</td>\n",
       "      <td>-0.575796</td>\n",
       "      <td>-1.181286</td>\n",
       "      <td>...</td>\n",
       "      <td>0.182677</td>\n",
       "      <td>-0.103928</td>\n",
       "      <td>0.136567</td>\n",
       "      <td>-0.002206</td>\n",
       "      <td>0.056856</td>\n",
       "      <td>0.035222</td>\n",
       "      <td>-0.147492</td>\n",
       "      <td>-0.098785</td>\n",
       "      <td>0.178695</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927</th>\n",
       "      <td>6.346374</td>\n",
       "      <td>-3.865858</td>\n",
       "      <td>6.209416</td>\n",
       "      <td>1.247192</td>\n",
       "      <td>0.112905</td>\n",
       "      <td>0.278156</td>\n",
       "      <td>-0.507665</td>\n",
       "      <td>-0.342197</td>\n",
       "      <td>-0.152731</td>\n",
       "      <td>-3.172417</td>\n",
       "      <td>...</td>\n",
       "      <td>0.065189</td>\n",
       "      <td>0.019135</td>\n",
       "      <td>-0.119160</td>\n",
       "      <td>0.090792</td>\n",
       "      <td>-0.125609</td>\n",
       "      <td>0.065373</td>\n",
       "      <td>-0.100236</td>\n",
       "      <td>0.039256</td>\n",
       "      <td>-0.039439</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>928 rows × 401 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "0   -0.457260  0.433402 -0.951313 -0.118767  0.372034  1.917341  0.503248   \n",
       "1   -1.482871 -3.923833  1.001672  1.151786 -0.105634  0.935446  0.061538   \n",
       "2    6.524849 -0.064380 -2.177830 -0.744632 -3.160053 -2.079289  1.991373   \n",
       "3   -4.626337 -0.901579 -1.351431 -1.767117  0.870166  2.210025  3.710694   \n",
       "4   -3.863027 -2.805259 -0.428457 -0.482452  0.231498  1.977056  1.605510   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "923 -3.365774 -2.530997 -0.781861  0.567793  1.960806  2.319130  3.632739   \n",
       "924 -6.340754  0.634164 -3.259888  2.894988  2.625952 -1.435685  1.570297   \n",
       "925 -2.106097  4.489429  4.740686  2.521037  1.810682  1.252919 -0.925497   \n",
       "926 -1.091978  7.063249  2.692234 -0.543705 -0.443376  1.028903 -1.749000   \n",
       "927  6.346374 -3.865858  6.209416  1.247192  0.112905  0.278156 -0.507665   \n",
       "\n",
       "            7         8         9  ...       391       392       393  \\\n",
       "0   -0.996278 -2.306744 -1.294890  ... -0.001274  0.137547 -0.054768   \n",
       "1    1.340459 -4.633610 -0.483502  ...  0.065975  0.004736  0.199824   \n",
       "2    2.629458  1.251710  1.656291  ...  0.125212 -0.007987  0.003295   \n",
       "3   -1.478115 -2.406238 -3.754295  ... -0.004587 -0.201108  0.008220   \n",
       "4   -0.603866 -1.312170  3.553396  ... -0.082930  0.000911 -0.015292   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "923  2.072028  0.318600  1.423024  ...  0.009309 -0.060514  0.149956   \n",
       "924  0.456509 -0.855480  0.523852  ...  0.200018  0.221275  0.268479   \n",
       "925  2.413092 -0.426080 -2.335654  ...  0.039288 -0.030993 -0.061379   \n",
       "926  0.469497 -0.575796 -1.181286  ...  0.182677 -0.103928  0.136567   \n",
       "927 -0.342197 -0.152731 -3.172417  ...  0.065189  0.019135 -0.119160   \n",
       "\n",
       "          394       395       396       397       398       399  target  \n",
       "0   -0.185737  0.108261 -0.044993 -0.069940 -0.033743  0.068947       2  \n",
       "1   -0.007765 -0.104836 -0.119618 -0.078848  0.025403 -0.049254       2  \n",
       "2   -0.059482  0.083645  0.043782 -0.047199 -0.097477 -0.056039       2  \n",
       "3    0.045321  0.022103 -0.052815 -0.030927 -0.099674 -0.110996       2  \n",
       "4    0.048979  0.103508 -0.232134  0.042196  0.165128 -0.068009       2  \n",
       "..        ...       ...       ...       ...       ...       ...     ...  \n",
       "923  0.077576 -0.041027 -0.025089 -0.085983 -0.044497  0.243577       3  \n",
       "924  0.089746 -0.050179 -0.012088  0.173675  0.037648 -0.061817       3  \n",
       "925  0.066061 -0.091756  0.094224  0.183882  0.008111 -0.095113       3  \n",
       "926 -0.002206  0.056856  0.035222 -0.147492 -0.098785  0.178695       3  \n",
       "927  0.090792 -0.125609  0.065373 -0.100236  0.039256 -0.039439       3  \n",
       "\n",
       "[928 rows x 401 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now Perform Dimensionality reduction (PCA) on that Dataframe and check\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#do PCA and choose componeents as 400\n",
    "pca = PCA(n_components=400)\n",
    "x_pca = pca.fit_transform(test_final)\n",
    "x_pca = pd.DataFrame(x_pca)\n",
    "\n",
    "# Calculate the variance explained by priciple components\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "print('Variance of each component:', pca.explained_variance_ratio_)\n",
    "print('\\n Total Variance Explained:', round(sum(list(pca.explained_variance_ratio_))*100, 2))\n",
    "\n",
    "#store the new pca generated dimensions in a dataframe\n",
    "#store the new pca generated dimensions in a dataframe\n",
    "pca_df = pd.DataFrame(data = x_pca)\n",
    "target = pd.Series(result_df.iloc[:,-1], name='target')\n",
    "final_result_df = pd.concat([pca_df, target], axis=1)\n",
    "final_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Rfdk0yXlO6rz"
   },
   "outputs": [],
   "source": [
    "#save to dimensionally reduced csv file\n",
    "final_result_df.to_csv(\"pca_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n18ljrh0KJB3",
    "outputId": "7a6f991c-c1e9-430e-c97c-a4d8c2542876"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PCA_ECG.pkl']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "#save the PCA model\n",
    "joblib_file='PCA_ECG.pkl'\n",
    "joblib.dump(pca,joblib_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l93cv3DXjw2A"
   },
   "source": [
    "#### **TRYING DIFFERENT ML MODELS ON THE ALL 12 LEADS COMBINED FILE WITHOUT DIMENSIONALITY REDUCTION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eLAdjoV0j5oS"
   },
   "source": [
    "##### **KNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Np5A0l30TYEI",
    "outputId": "9e5f4a0e-90d6-4853-a687-f4e69a70de1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.793010752688172\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.65      0.76       105\n",
      "           1       0.95      0.91      0.93        94\n",
      "           2       0.70      0.86      0.77       112\n",
      "           3       0.65      0.74      0.69        61\n",
      "\n",
      "    accuracy                           0.79       372\n",
      "   macro avg       0.80      0.79      0.79       372\n",
      "weighted avg       0.81      0.79      0.79       372\n",
      "\n",
      "Tuned Model Parameters: {'knn__n_neighbors': 1}\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules for ML model\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Setup the pipeline steps:\n",
    "steps = [('knn', KNeighborsClassifier())]\n",
    "        \n",
    "# Create the pipeline: pipeline \n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# have paased less range value of hyperparamter since i'm using free tier version of google colab.\n",
    "k_range = list(range(1, 30))\n",
    "parameters = dict(knn__n_neighbors=k_range)\n",
    "\n",
    "#input\n",
    "X = final_result_df.iloc[:,:-1]\n",
    "\n",
    "#target\n",
    "y=final_result_df.iloc[:,-1]\n",
    "\n",
    "# Create train and test sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.4,random_state=42)\n",
    "\n",
    "#increasing cv score takes lot of time in gooogle colab, so kept it just 2.\n",
    "cv = GridSearchCV(pipeline,parameters,cv=2)\n",
    "\n",
    "cv.fit(X_train,y_train)\n",
    "\n",
    "# Predict the labels of the test set: y_pred\n",
    "y_pred = cv.predict(X_test)\n",
    "\n",
    "Knn_Accuracy = cv.score(X_test, y_test)\n",
    "\n",
    "# Compute and print metrics\n",
    "print(\"Accuracy: {}\".format(Knn_Accuracy))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Tuned Model Parameters: {}\".format(cv.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "METf9MJdkESh"
   },
   "source": [
    "##### **LOGISTIC REGRESSION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "3qBayEUskDfc"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Setup the pipeline steps:\n",
    "steps = [('lr', LogisticRegression())]\n",
    "        \n",
    "# Create the pipeline: pipeline \n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "#input\n",
    "X = final_result_df.iloc[:,:-1]\n",
    "\n",
    "#target\n",
    "y=final_result_df.iloc[:,-1]\n",
    "\n",
    "#parameters for gridsearchcv\n",
    "c_space = np.logspace(-4, 4, 10)\n",
    "parameters = {'lr__C': c_space,'lr__penalty': ['l2']} \n",
    "\n",
    "# Create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.4,random_state=42)\n",
    "\n",
    "#call GridSearchCV and set crossvalscore to 2\n",
    "cv = GridSearchCV(pipeline,parameters,cv=2)\n",
    "\n",
    "cv.fit(X_train,y_train)\n",
    "\n",
    "# Predict the labels of the test set: y_pred\n",
    "y_pred = cv.predict(X_test)\n",
    "LR_Accuracy = cv.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_xV9YlHZktj9",
    "outputId": "b6fc9558-a064-407b-ced4-c81e3aefbcee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7768817204301075\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.57      0.68       105\n",
      "           1       0.83      0.91      0.87        94\n",
      "           2       0.82      0.86      0.84       112\n",
      "           3       0.59      0.77      0.67        61\n",
      "\n",
      "    accuracy                           0.78       372\n",
      "   macro avg       0.77      0.78      0.76       372\n",
      "weighted avg       0.79      0.78      0.77       372\n",
      "\n",
      "Tuned Model Parameters: {'lr__C': 0.3593813663804626, 'lr__penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "# Compute and print metrics\n",
    "print(\"Accuracy: {}\".format(LR_Accuracy))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Tuned Model Parameters: {}\".format(cv.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZOJDLUA6p5Xd"
   },
   "source": [
    "##### **SVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yG0rz8crlJK5",
    "outputId": "a560d998-8fd8-4fe9-bac6-be22cda925b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9051724137931034\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.92      0.86       119\n",
      "           1       1.00      1.00      1.00       125\n",
      "           2       0.91      0.89      0.90       140\n",
      "           3       0.93      0.78      0.84        80\n",
      "\n",
      "    accuracy                           0.91       464\n",
      "   macro avg       0.91      0.89      0.90       464\n",
      "weighted avg       0.91      0.91      0.91       464\n",
      "\n",
      "Tuned Model Parameters: {'SVM__C': 10, 'SVM__gamma': 0.01}\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules for ML model\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Setup the pipeline\n",
    "steps = [('SVM', SVC())]\n",
    "\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "#input\n",
    "X = final_result_df.iloc[:,:-1]\n",
    "\n",
    "#target\n",
    "y=final_result_df.iloc[:,-1]\n",
    "\n",
    "# Specify the hyperparameter space, if we increase the penalty(c) and gamma value the accurancy can be increased.\n",
    "#since it takes lots of time in google colab provided only a single value\n",
    "parameters = {'SVM__C':[1, 10, 100],\n",
    "              'SVM__gamma':[0.1, 0.01]}\n",
    "\n",
    "# Create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.5,random_state=21)\n",
    "\n",
    "cv = GridSearchCV(pipeline,parameters,cv=3)\n",
    "cv.fit(X_train,y_train)\n",
    "\n",
    "y_pred = cv.predict(X_test)\n",
    "SVM_Accuracy = cv.score(X_test, y_test)\n",
    "\n",
    "# Compute and print metrics\n",
    "SVM_Accuracy=cv.score(X_test, y_test)\n",
    "\n",
    "print(\"Accuracy: {}\".format(SVM_Accuracy))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Tuned Model Parameters: {}\".format(cv.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eqxwP-4vIbbm"
   },
   "source": [
    "### **XGBOOST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "4xWCZUjyIWoJ"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, objective=&#x27;multi:softprob&#x27;, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, objective=&#x27;multi:softprob&#x27;, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=None, n_jobs=None,\n",
       "              num_parallel_tree=None, objective='multi:softprob', ...)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = XGBClassifier()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "bzdU_OL7IY0U"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3OY33WUdIapb",
    "outputId": "40e112ea-0807-47bc-a873-0bdeeca4f0c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8663793103448276\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.71      0.76       119\n",
      "           1       0.98      1.00      0.99       125\n",
      "           2       0.79      0.90      0.84       140\n",
      "           3       0.90      0.82      0.86        80\n",
      "\n",
      "    accuracy                           0.87       464\n",
      "   macro avg       0.87      0.86      0.86       464\n",
      "weighted avg       0.87      0.87      0.86       464\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: {}\".format(accuracy))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "izUvTTDBp-Vy"
   },
   "source": [
    "#### **SAVING A VERY BASIC ML MODEL AND USING IT ON REALTIME PIPELINE TO CHECK WORKING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mN7RIL_3PnJb",
    "outputId": "6bcad7b1-cfbc-4066-f8c2-5404e4e6da11"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['knn_model_test.pkl']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the necessary modules for ML model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import joblib\n",
    "#input\n",
    "X = final_result_df.iloc[:,:-1]\n",
    "#target\n",
    "y=final_result_df.iloc[:,-1]\n",
    "\n",
    "# Create train and test sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.4,random_state=42)\n",
    "\n",
    "knn =  KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "knn.fit(X_train,y_train)\n",
    "\n",
    "joblib_file='knn_model_test.pkl'\n",
    "joblib.dump(knn,joblib_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cNiBzgDv_-oA",
    "outputId": "e68d2bfc-1935-404e-de27-3628e2d9e0c5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['svm_model_test.pkl']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the necessary modules for ML model\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "#input\n",
    "X = pd.read_csv('final_1D.csv',header=None)\n",
    "\n",
    "#target\n",
    "y=final_result_df.iloc[:,-1]\n",
    "\n",
    "# Create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.5,random_state=21)\n",
    "\n",
    "svm=SVC(C=10,gamma=0.01)\n",
    "\n",
    "svm.fit(X_train,y_train)\n",
    "\n",
    "joblib_file='svm_model_test.pkl'\n",
    "joblib.dump(svm,joblib_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vmKRxfCBH7Yo"
   },
   "source": [
    "### **ENSEMBLE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "7V2P-WtiYZ4f"
   },
   "outputs": [],
   "source": [
    "# Importing required modules\n",
    "from sklearn import linear_model, tree, ensemble\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "eA1TkdrcYZ4g"
   },
   "outputs": [],
   "source": [
    "#input\n",
    "X = final_result_df.iloc[:,0:-1]\n",
    "\n",
    "#target\n",
    "y=final_result_df.iloc[:,-1]\n",
    "\n",
    "# Create train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "A4caey39YZ4f"
   },
   "outputs": [],
   "source": [
    "# Stacking of ML Models\n",
    "eclf = VotingClassifier(estimators=[ \n",
    "    ('SVM', SVC(probability=True)),\n",
    "    ('knn', KNeighborsClassifier()),\n",
    "    ('rf', ensemble.RandomForestClassifier()),\n",
    "    ('bayes',GaussianNB()),\n",
    "    ('logistic',LogisticRegression()),\n",
    "    ], voting='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "neqxcy3GqS91",
    "outputId": "5acb921a-9719-4836-8ef2-4b7ba370e27b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SVM__C': 100, 'SVM__gamma': 0.1, 'knn__n_neighbors': 5, 'rf__n_estimators': 400}\n",
      "Accuracy: 0.921146953405018\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.93      0.96        80\n",
      "           1       1.00      1.00      1.00        72\n",
      "           2       0.83      0.92      0.87        79\n",
      "           3       0.84      0.79      0.82        48\n",
      "\n",
      "    accuracy                           0.92       279\n",
      "   macro avg       0.92      0.91      0.91       279\n",
      "weighted avg       0.92      0.92      0.92       279\n",
      "\n",
      "{'SVM__C': 100, 'SVM__gamma': 0.1, 'knn__n_neighbors': 5, 'rf__n_estimators': 400}\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter Tuning using gridSearch\n",
    "params = {'SVM__C':[1, 10, 100],\n",
    "          'SVM__gamma':[0.1, 0.01],\n",
    "          'knn__n_neighbors': [1,3,5],\n",
    "          'rf__n_estimators':[300, 400],\n",
    "          }\n",
    "\n",
    "grid = GridSearchCV(estimator=eclf, param_grid=params, cv=5)\n",
    "voting_clf = grid.fit(X_train, y_train)\n",
    "\n",
    "print(grid.best_params_)\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "\n",
    "# Compute and print metrics\n",
    "Voting_Accuracy=voting_clf.score(X_test, y_test)\n",
    "\n",
    "print(\"Accuracy: {}\".format(Voting_Accuracy))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(voting_clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "91lBO_ZUYZ4j"
   },
   "outputs": [],
   "source": [
    "# open a file, where you ant to store the data\n",
    "file = open('Heart_Disease_Prediction_using_ECG.pkl', 'wb')\n",
    "# dump information to that file\n",
    "pickle.dump(voting_clf, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Emu__tW7qP9b"
   },
   "source": [
    "## SAVE AND USE THE ABOVE MODEL IN THE STREAMLIT APP : **https://colab.research.google.com/drive/139YVmcUBCiP52J2sX3QE_eiu2sukVgpn?usp=sharing**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Merging_Scaled_1D_&_Trying_Different_CLassification_ML_Models_.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
